# AI Project Orchestrator - Development Progress & Handover

## Developer Context
**Profile**: Senior .NET developer (15+ years experience) working as university employee with access to Canvas LMS API. Primary concern is preparing for senior developer interviews at AI-forward companies by rebuilding code evaluation skills that have atrophied due to heavy AI assistant usage.

**Core Challenge**: Developer has become "too lazy to analyze code" generated by AI tools (primarily uses Cline with Qwen3-coder), trusting AI output blindly. This creates vulnerability in senior interviews where code evaluation skills are critical.

**Communication Style**: Prefers honest, direct feedback without flattery. Values objective analysis over encouragement. Learning approach is hands-on experimentation with complex, exciting projects.

## Ultimate Goal & Vision
Build an **AI Project Orchestrator** that automates the entire development workflow from brainstorming to working code, while simultaneously serving as a learning vehicle to rebuild systematic code evaluation skills through structured AI collaboration.

### Primary Objectives:
1. **Interview Preparation**: Build evaluation skills for senior .NET developer interviews (3-4 month timeline)
2. **Workflow Automation**: Eliminate 80% of manual prompt engineering through systematic AI orchestration
3. **Model Optimization**: Solve the "which AI model for which task" decision problem with intelligent routing
4. **Learning Platform**: Practice modern .NET patterns (background processing, microservices, async patterns, etc.)

## System Architecture Vision

### Core Pipeline Flow
```
High-Level Idea 
↓ (Claude Sonnet)
Requirements Analysis → Extract functional/non-functional requirements
↓ (Claude Sonnet)  
Project Planning → Generate roadmaps, milestones, architecture decisions
↓ (Claude Sonnet)
Story Generation → Create implementable user stories with acceptance criteria
↓ (Context Engineering)
Context Management → Gather relevant examples, documentation, patterns
↓ (Model Orchestration)
Implementation → Route to optimal models (Qwen3-coder/DeepSeek for coding)
↓ (Quality Assessment)
Human Approval → Structured feedback loops at each stage
```

### AI Model Infrastructure
- **Claude API**: Brainstorming, requirements analysis, planning
- **LM Studio** (http://100.74.43.85:1234): Local model hosting
- **OpenRouter API**: Flexible model access with API key
- **Current Models**: Qwen3-coder, DeepSeek, Granite, multiple others in collection

### Technical Stack Decisions
- **.NET 9 Web API**: Clean Architecture with eventual microservices extraction
- **PostgreSQL**: Database (existing installation at 192.168.68.112)
- **Entity Framework Core**: ORM with code-first approach
- **Docker**: Containerized development and deployment
- **GitHub Actions**: CI/CD automation
- **Sub-Agent Instructions**: Individual .md files for each service persona

## Development Process Philosophy

### Key Principles Applied
1. **Structured LLM Collaboration**: Human maintains control with AI handling heavy lifting
2. **Quality Feedback Loops**: Manual approval/refinement at each pipeline stage
3. **Context Window Optimization**: Keep utilization under 40% through compression strategies
4. **Test-Driven Development**: Generate tests first, then implementation
5. **Chain of Thought**: Explicit step-by-step reasoning in AI interactions
6. **Start Simple, Iterate**: Begin with minimal prompts, refine based on failures

### Current Workflow
Developer typically follows: Brainstorm → Vision/Plan/Roadmap → User Stories → Implementation (one story at a time) → Context documents (~10k characters in markdown)

## Current System State

### Project Structure (Completed)
```
C:\\git\\AIProjectOrchestrator\\
├── src/
│   ├── AIProjectOrchestrator.API/           # Web API layer
│   ├── AIProjectOrchestrator.Application/   # Business logic, services  
│   ├── AIProjectOrchestrator.Domain/        # Domain entities, interfaces
│   └── AIProjectOrchestrator.Infrastructure/ # Data access, external APIs
├── tests/
│   ├── AIProjectOrchestrator.UnitTests/
│   └── AIProjectOrchestrator.IntegrationTests/
├── docs/                                    # Documentation folder
│   ├── user-stories/                        # User stories and requirements
│   ├── architecture/                        # Architecture decisions, diagrams  
│   └── setup/                               # Setup and deployment guides
├── Dockerfile                              # Multi-stage .NET 9 API build
├── docker-compose.yml                       # PostgreSQL + API services
├── .github/workflows/                       # CI/CD automation
├── .gitignore                              # .NET specific ignores
└── README.md                               # Setup instructions
```

### Infrastructure Status
- **Solution Build**: ✅ Builds without errors (`dotnet build` successful)
- **Docker Compose**: ✅ Working PostgreSQL + API containers
- **Health Checks**: ✅ API responds on http://localhost:8080/health
- **Database**: ✅ PostgreSQL initializing and accepting connections
- **CI/CD Pipeline**: ✅ GitHub Actions configured
- **Clean Architecture**: ✅ Proper layer separation and dependencies

### Foundation Documents Created
- **vision.md**: Project vision, requirements, success criteria, architecture principles
- **Qwen.md**: Comprehensive AI coding assistant instructions for Clean Architecture .NET development
- **Sample Entity**: Project entity with repository pattern (scaffolding, can be modified/removed)

## Completed User Stories

### US-000: Development Environment Setup ✅
**Status**: COMPLETED
**Implementation**: Created by Qwen3-coder via structured prompt
**Deliverables**:
- Complete Clean Architecture solution structure
- Docker containerization with PostgreSQL dependency  
- GitHub Actions CI/CD pipeline
- Health checks endpoint (/health)
- Basic logging with Serilog
- Entity Framework Core setup with sample Project entity
- Unit and integration test projects
- Comprehensive documentation structure

**Quality Assessment**: Infrastructure is solid, builds successfully, containers run properly. Sample Project entity demonstrates proper patterns but isn't needed for orchestrator functionality.

## Immediate Next Steps (US-001)

### US-001: Service Configuration System
**Priority**: HIGH - Core infrastructure for AI orchestration
**Requirements**:
- Create `/Instructions/` folder for sub-agent instruction files
- Implement service to load and manage instruction files (RequirementsAnalyst.md, ProjectPlanner.md, etc.)
- Support hot-reloading of instruction files during development
- Validation system for instruction file format and content
- Configuration system for instruction file management

**Technical Implementation Needs**:
- IInstructionService interface in Domain layer
- InstructionService implementation in Application layer
- File system watcher for hot-reloading
- Dependency injection registration
- Unit tests for instruction loading and validation

## Pending User Stories (Not Yet Implemented)

### Epic 1: Core Infrastructure
- **US-002: Multi-Provider Model Client** - Abstraction layer for Claude/LM Studio/OpenRouter with dynamic routing
- **US-003: Quality Feedback Loop Framework** - Human review/approval workflow system

### Epic 2: Requirements → Stories Pipeline  
- **US-004: Requirements Analysis Service** - Claude-powered analysis of high-level project ideas
- **US-005: Project Planning Service** - Roadmap and milestone generation
- **US-006: Story Generation Service** - User story creation with acceptance criteria

### Future Epics (Planned)
- **Epic 3: Implementation Pipeline** - Code generation with TDD approach
- **Epic 4: Context Management** - Smart context optimization and compression
- **Epic 5: Model Intelligence** - Performance tracking and optimal routing
- **Epic 6: Advanced Features** - Analytics, learning, template system

## AI Integration Architecture (Planned)

### Sub-Agent Instruction System
Each AI service will have dedicated instruction files:
- `RequirementsAnalyst.md` - Business analysis specialist
- `ProjectPlanner.md` - Technical project planning expert  
- `StoryGenerator.md` - User story creation specialist
- `CodeReviewer.md` - Quality assessment expert
- `ArchitecturalGuide.md` - System design advisor

### Model Routing Strategy
- **Claude Sonnet**: Brainstorming, requirements analysis, planning
- **Qwen3-coder**: Code generation, technical implementation
- **DeepSeek**: Alternative coding model for comparison/validation
- **Intelligent Selection**: Based on task type, historical performance, availability

### Context Management Strategy
- Automatic context compression to maintain <40% window utilization
- Smart summarization of previous pipeline stages
- External file storage for large context documents
- Context relevance scoring and filtering

## Development Environment Setup

### Local Development
```bash
# Navigate to project
cd C:\git\AIProjectOrchestrator

# Build solution
dotnet build

# Run with Docker
docker-compose up -d

# View logs  
docker-compose logs -f api

# Run tests
dotnet test
```

### Key Configuration Files
- **appsettings.json**: Basic configuration (no secrets)
- **Dockerfile**: Multi-stage .NET 9 API build
- **docker-compose.yml**: PostgreSQL + API container orchestration
- **.github/workflows/**: CI/CD automation (build, test, deploy)

## Critical Implementation Notes

### Code Evaluation Learning Approach
The primary goal is rebuilding systematic code evaluation skills. Each AI-generated component should be reviewed for:
1. **Structural Quality**: Interface usage, method count, class responsibility
2. **Dependency Injection**: Proper constructor injection, lifetime management
3. **Clean Architecture**: Correct layer separation, dependency flow
4. **Enterprise Patterns**: Error handling, logging, async/await usage
5. **Security Considerations**: Input validation, secret management

### Quality Gates Philosophy
- Never trust AI-generated code without systematic review
- Each pipeline stage requires human approval before progression
- Focus on architectural decisions and implementation patterns
- Build evaluation checklists for consistent code review

### Learning Objectives Tracking
- Practice background processing evaluation (Hangfire, async patterns)
- Develop microservices assessment skills
- Build confidence in architectural pattern recognition
- Prepare for senior interview technical discussions

## Success Metrics
- **Technical**: 80% reduction in manual prompt engineering time
- **Learning**: Ability to quickly evaluate AI-generated enterprise patterns
- **Interview Prep**: Confident technical discussions about modern .NET architecture
- **System**: Reliable automation from requirements to working code

## Next Session Handover Prompt

Continue development of the AI Project Orchestrator with focus on implementing **US-001: Service Configuration System**. The developer needs to practice systematic code evaluation skills while building the foundation for AI orchestration.

**Context**: This is a learning project for senior .NET interview preparation. Developer has 15+ years experience but needs to rebuild code evaluation skills after becoming dependent on AI assistants. Approach should be hands-on with complex, interesting challenges.

**Current Status**: Clean Architecture foundation is complete and working (builds, containers run, tests pass). Ready to implement AI-specific infrastructure.

**Communication Style**: Direct, honest feedback without flattery. Focus on objective technical assessment and code quality evaluation skills.

**Immediate Goal**: Implement service configuration system that loads instruction files for AI sub-agents, while practicing systematic evaluation of AI-generated code quality.