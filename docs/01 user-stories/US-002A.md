## Refined US-002A: Multi-Provider AI Client Interface

**As a** developer  
**I want** a unified interface to call AI models across different providers  
**So that** services can make AI requests without provider-specific code

### **Acceptance Criteria**

#### Core Interface Implementation
- [ ] Define `IAIClient` interface with signature: `Task<AIResponse> CallAsync(AIRequest request, CancellationToken cancellationToken)`
- [ ] Create `AIRequest` model with properties: Prompt, SystemMessage, ModelName, Temperature, MaxTokens
- [ ] Create `AIResponse` model with properties: Content, TokensUsed, ProviderName, IsSuccess, ErrorMessage
- [ ] Implement three concrete clients: `ClaudeClient`, `LMStudioClient`, `OpenRouterClient`

#### Configuration & Infrastructure
- [ ] Add provider configuration section to appsettings.json with API keys, base URLs, and timeouts
- [ ] Create `AIProviderSettings` configuration class with `IOptions<T>` pattern
- [ ] Register all clients as Singleton services in DI container
- [ ] Implement structured logging for all AI calls with correlation IDs and performance metrics

#### Error Handling & Resilience
- [ ] Create custom exception types: `AIProviderException`, `AIRateLimitException`, `AITimeoutException`
- [ ] Implement basic retry logic for transient failures (network timeouts, temporary unavailability)
- [ ] Handle HTTP status codes appropriately (401, 429, 500, etc.)
- [ ] Graceful degradation when providers are unavailable

#### Provider-Specific Implementation
- [ ] **Claude**: Implement Anthropic API format with proper authentication headers
- [ ] **LM Studio**: Implement OpenAI-compatible format for local endpoint calls
- [ ] **OpenRouter**: Implement OpenAI format with provider routing headers
- [ ] Support different model names per provider (claude-sonnet-4, qwen-coder, etc.)

### **Testing Requirements**

#### Unit Tests
- [ ] Test `AIRequest` and `AIResponse` model validation
- [ ] Test each client's request formatting and response parsing
- [ ] Test error handling scenarios (network failures, API errors, timeouts)
- [ ] Test retry logic with mock HTTP responses
- [ ] Test configuration loading and validation
- [ ] Achieve minimum 80% code coverage for all client implementations

#### Integration Tests  
- [ ] Test actual API calls to each provider with real endpoints
- [ ] Test authentication and authorization flows
- [ ] Test timeout and cancellation token behavior
- [ ] Test rate limiting and retry scenarios
- [ ] Verify structured logging output format and correlation IDs

#### Mock/Test Infrastructure
- [ ] Create mock HTTP client for unit testing
- [ ] Create test configuration with fake API keys and endpoints
- [ ] Create sample AI responses for different success/failure scenarios
- [ ] Set up test categorization for unit vs integration tests

### **Technical Specifications**

#### HTTP Client Configuration
- Use `IHttpClientFactory` for proper HttpClient lifecycle management
- Configure per-provider timeouts (Claude: 30s, LM Studio: 60s, OpenRouter: 30s)
- Implement request/response logging with sanitized API keys

#### Retry Logic
- Maximum 3 retry attempts for transient failures
- Exponential backoff: 1s, 2s, 4s delays
- Only retry on specific HTTP status codes: 408, 429, 500, 502, 503, 504

#### Configuration Structure
```json
{
  "AIProviders": {
    "Claude": {
      "ApiKey": "sk-...",
      "BaseUrl": "https://api.anthropic.com",
      "TimeoutSeconds": 30,
      "MaxRetries": 3
    },
    "LMStudio": {
      "BaseUrl": "http://100.74.43.85:1234",
      "TimeoutSeconds": 60,
      "MaxRetries": 2
    },
    "OpenRouter": {
      "ApiKey": "sk-or-...",
      "BaseUrl": "https://openrouter.ai/api/v1",
      "TimeoutSeconds": 30,
      "MaxRetries": 3
    }
  }
}
```

### **Definition of Done**
- [ ] All interfaces and implementations compile without errors
- [ ] All clients registered in DI container with proper lifetimes
- [ ] Unit tests pass with >80% code coverage
- [ ] Integration tests successfully call real API endpoints
- [ ] Configuration properly loads from appsettings.json
- [ ] Structured logging implemented with correlation tracking
- [ ] Error handling covers all specified failure scenarios
- [ ] All existing tests continue to pass
- [ ] Docker containers build and run successfully
- [ ] API documentation updated to reflect new AI client capabilities

This provides comprehensive implementation guidance while maintaining focus on the core multi-provider abstraction pattern.