## US-003A: Simple Output Review API

**As a** developer using the AI Project Orchestrator  
**I want** REST API endpoints to submit AI outputs for human review and approval  
**So that** I can implement human-in-the-loop workflows in the AI orchestration pipeline

### **Context**
With the multi-provider AI client system (US-002A) now working, we need human oversight at each pipeline stage. This API enables submitting AI outputs (from RequirementsAnalyst, ProjectPlanner, etc.) for human review before proceeding to the next orchestration step.

### **Acceptance Criteria**

#### Core API Endpoints
- [ ] **POST /api/review/submit** - Submit AI output for human review
  - Request: `SubmitReviewRequest` with ServiceName, Content, CorrelationId, PipelineStage, Metadata
  - Response: Review ID (Guid) and initial status
  - Integration: Use existing correlation ID system from AI client calls
- [ ] **GET /api/review/{id}** - Retrieve pending review with context
  - Response: Complete review details including original AI request context
  - Include: submission timestamp, AI provider used, response time metadata
- [ ] **POST /api/review/{id}/approve** - Approve and mark ready for next stage
  - Log decision with existing structured logging system
  - Return: success status and next pipeline stage information
- [ ] **POST /api/review/{id}/reject** - Reject with improvement feedback
  - Request: `ReviewDecisionRequest` with reason and specific feedback
  - Store feedback for potential AI instruction refinement

#### Data Models & Storage
- [ ] Create `ReviewSubmission` model integrating with existing `AIResponse` structure
- [ ] Create `ReviewDecision` model for tracking approval/rejection decisions
- [ ] **In-memory storage only**: Use `ConcurrentDictionary<Guid, ReviewSubmission>` 
- [ ] Background cleanup service for reviews >24 hours old (use existing background service patterns)
- [ ] Thread-safe operations to handle concurrent review submissions

#### Integration Requirements
- [ ] Integrate with existing `ILogger<T>` structured logging (match AI client patterns)
- [ ] Use existing correlation ID system from `BaseAIClient` implementation
- [ ] Follow established API controller patterns from `AITestController`
- [ ] Add review endpoints to existing health check system
- [ ] Use existing validation patterns and error handling from AI clients

### **Technical Implementation Details**

#### Request/Response Models
```csharp
public class SubmitReviewRequest
{
    public string ServiceName { get; set; } = string.Empty; // "RequirementsAnalyst", "ProjectPlanner", etc.
    public string Content { get; set; } = string.Empty;     // AI output content
    public string CorrelationId { get; set; } = string.Empty; // From AI client call
    public string PipelineStage { get; set; } = string.Empty; // "Analysis", "Planning", "Stories"
    public AIRequest? OriginalRequest { get; set; }          // Original AI request for context
    public AIResponse? AIResponse { get; set; }              // Full AI response details
    public Dictionary<string, object> Metadata { get; set; } = new();
}

public class ReviewDecisionRequest  
{
    public string Reason { get; set; } = string.Empty;
    public string Feedback { get; set; } = string.Empty;
    public Dictionary<string, string> InstructionImprovements { get; set; } = new(); // For instruction refinement
}
```

#### Service Layer Integration
- [ ] Create `IReviewService` interface in Domain layer (following `IAIClient` pattern)
- [ ] Implement `ReviewService` in Application layer (following `InstructionService` pattern)
- [ ] Register as Singleton in DI container with existing AI services
- [ ] Use existing `IOptions<T>` configuration pattern for review settings

#### Controller Implementation
- [ ] Create `ReviewController` following `AITestController` patterns
- [ ] Use existing `[ApiController]` and routing conventions
- [ ] Implement same error handling patterns as AI client controllers
- [ ] Include OpenAPI documentation attributes matching existing controllers

### **Testing Requirements**

#### Unit Tests
- [ ] Test models, service operations, and controller endpoints
- [ ] Mock existing AI client integration points
- [ ] Test concurrent access scenarios with `ConcurrentDictionary`
- [ ] Follow existing test project structure and patterns
- [ ] Target >80% code coverage matching AI client test standards

#### Integration Tests  
- [ ] Test complete workflow: AI call → submit for review → approve/reject
- [ ] Test correlation ID propagation through review process
- [ ] Test integration with existing health check system
- [ ] Use existing integration test patterns and categories

### **Configuration Integration**
```json
{
  "ReviewSettings": {
    "MaxConcurrentReviews": 100,
    "ReviewTimeoutHours": 24,
    "CleanupIntervalMinutes": 60
  }
}
```

### **Definition of Done**
- [ ] All endpoints implemented following existing API patterns
- [ ] Models created with proper validation using existing validation approaches
- [ ] In-memory storage implemented with thread safety
- [ ] Integration with existing logging, correlation ID, and health check systems
- [ ] Unit and integration tests >80% coverage following existing test patterns  
- [ ] Controller added to existing OpenAPI documentation
- [ ] All existing tests continue to pass (US-001, US-002A)
- [ ] Docker containers build and run with new endpoints
- [ ] Health checks include review system status

### **Integration Points with Existing System**
1. **AI Client Integration**: Reviews submitted after `IAIClient.CallAsync()` calls
2. **Instruction System**: Review feedback can inform instruction file improvements
3. **Correlation Tracking**: Maintain correlation IDs from AI requests through review process
4. **Configuration System**: Use existing `IOptions<T>` patterns
5. **Logging System**: Extend existing structured logging with review events
6. **Health Monitoring**: Include review system in existing health check infrastructure

This story builds directly on the working US-002A infrastructure while creating the essential human oversight capability for AI orchestration workflows.